{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eddf1c18-51b1-463b-b776-13141de692ea",
   "metadata": {},
   "source": [
    "<h1>Building and Training a Simple Neural Network in PyTorch</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ad96c7-b01d-476c-9cd4-40e9d7578ae6",
   "metadata": {},
   "source": [
    "Now that we have a grasp on tensors and autograd, let's build a simple neural network from scratch using PyTorch.<br>\n",
    "We'll cover defining the <b>network architecture</b>,  <b>preparing the dataset</b>, <b>defining the loss function</b> and <b>optimizer</b>, and <b>training the model</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21214027-2250-4e72-b9ad-3c24f9e3b98b",
   "metadata": {},
   "source": [
    "Step-by-Step Guide to Building a Neural Network\n",
    "<ol>\n",
    "<li><b>Define the Neural Network Architecture:</b> Create a class inheriting from nn.Module.</li>\n",
    "<li><b>Prepare the Dataset:</b> Use PyTorch's Dataset and DataLoader classes.</li>\n",
    "<li><b>Define the Loss Function and Optimizer:</b> Use built-in loss functions and optimizers.</li>\n",
    "<li><b>Train the Model:</b> Implement the training loop.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e9bb1e-4bb1-41c9-90c6-61af0027c6d3",
   "metadata": {},
   "source": [
    "<h4>1. Define the Neural Network Architecture</h4>\n",
    "We'll define a simple feedforward neural network with one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "302bf66a-07f9-4dac-93d1-0c50c95d3b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (hidden): Linear(in_features=2, out_features=5, bias=True)\n",
      "  (output): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.hidden = nn.Linear(2, 5)  # Hidden layer with 5 neurons | 2 input features and 5 output features | The output 5 features will be input to next layer as 5 input featurs\n",
    "        self.output = nn.Linear(5, 1)  # Output layer | 5 input features and 1 output feature.    \n",
    "    '''\n",
    "    Modifying the number of neurons(i.e in above example it is 5) affects the model's capacity to learn and represent the data. \n",
    "    Case 1: If we Increasing the Number of Neurons\n",
    "            Effects:\n",
    "            a) Capacity: Increasing the number of neurons in the hidden layer increases the model's capacity to learn and represent more complex functions. \n",
    "            This can potentially improve performance on complex datasets.\n",
    "            b) Overfitting: A larger model can more easily overfit the training data, especially if the dataset is small or noisy. \n",
    "            Regularization techniques such as dropout or weight decay might be needed to mitigate this risk.\n",
    "            c)Computational Cost: More neurons mean more parameters to train, which increases the computational cost and memory usage during training and \n",
    "            inference.\n",
    "    \n",
    "    Case 2: Decreasing the Number of Neurons\n",
    "            Effects:\n",
    "            a) Capacity: Decreasing the number of neurons reduces the model's capacity to learn and represent complex functions. \n",
    "            This might lead to underfitting, where the model fails to capture important patterns in the data.\n",
    "            b) Generalization: A smaller model is less likely to overfit and may generalize better on simpler or smaller datasets.\n",
    "            c) Computational Cost: Fewer neurons mean fewer parameters to train, which reduces the computational cost and memory usage.\n",
    "\n",
    "    Choosing the Number of Neurons\n",
    "    The optimal number of neurons in the hidden layer depends on various factors:\n",
    "        a) Dataset Complexity: More complex datasets generally require more neurons to capture the underlying patterns.\n",
    "        b) Size of the Dataset: Larger datasets can support larger models without overfitting, while smaller datasets might require smaller models.\n",
    "        c) Model Generalization: Balancing the capacity of the model to avoid both underfitting and overfitting is crucial. Cross-validation and hyperparameter tuning can help find the optimal number of neurons.\n",
    "\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden(x))  # Apply ReLU activation\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the network\n",
    "net = SimpleNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e304e49-1450-48b1-933e-75f51c6e9cd2",
   "metadata": {},
   "source": [
    "<h4>2. Prepare the Dataset </h4>\n",
    "We'll create a simple synthetic dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f32b2786-3177-4f55-bdfc-7b4b9f53a15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: tensor([[2., 3.],\n",
      "        [1., 2.]])\n",
      "Target: tensor([[2.],\n",
      "        [1.]])\n",
      "Data: tensor([[4., 5.],\n",
      "        [3., 4.]])\n",
      "Target: tensor([[4.],\n",
      "        [3.]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create a synthetic dataset\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = torch.tensor([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0], [4.0, 5.0]]) # (4,2)\n",
    "        self.targets = torch.tensor([[1.0], [2.0], [3.0], [4.0]]) # (4,1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "# Instantiate the dataset and dataloader\n",
    "dataset = SimpleDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)  #  Loads the dataset with specified batch size and shuffling.\n",
    "\n",
    "for data, target in dataloader:\n",
    "    print(\"Data:\", data)\n",
    "    print(\"Target:\", target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c408de66-f918-47ad-b56d-f0d5838f4d31",
   "metadata": {},
   "source": [
    "<h4> 3. Define the Loss Function and Optimizer </h4>\n",
    "We'll use Mean Squared Error (MSE) as the loss function and Stochastic Gradient Descent (SGD) as the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6eed35d-972c-45b4-9bd7-44ca75161b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()                            # Mean Squared Error (MSE)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)    # Stochastic Gradient Descent (SGD) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5235f0fe-80e0-4a96-b731-c74dc9586227",
   "metadata": {},
   "source": [
    "<h4> 4. Train the Model </h4>\n",
    "Implement the training loop to train the model over multiple epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bab0c3c8-1214-4ed8-86d0-abbc0410409a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.0169\n",
      "Epoch [200/1000], Loss: 0.0013\n",
      "Epoch [300/1000], Loss: 0.0002\n",
      "Epoch [400/1000], Loss: 0.0002\n",
      "Epoch [500/1000], Loss: 0.0000\n",
      "Epoch [600/1000], Loss: 0.0000\n",
      "Epoch [700/1000], Loss: 0.0000\n",
      "Epoch [800/1000], Loss: 0.0000\n",
      "Epoch [900/1000], Loss: 0.0000\n",
      "Epoch [1000/1000], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data, target in dataloader:\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()     # Clears gradients to avoid accumulation.\n",
    "        \n",
    "        # Forward pass\n",
    "        output = net(data)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass (compute gradients)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:           # Print the loss at every 100th Epoch\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb443cd-cf50-487a-91bf-2b2b0a40e87c",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202292d3-60e4-41b4-a697-7565692151bc",
   "metadata": {},
   "source": [
    "<h4><b>*Additional</b> - Hyperparameter Tuning</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b159775-4768-416b-9a34-cd4f1ade2da5",
   "metadata": {},
   "source": [
    "As mentioned in comments in first section, the <b>number of neurons</b> in layers affects the model's performance. <br>\n",
    "So, to decide it's value is crucial. So either we can decide it by increasing numbers or hit and trial orr we can use \"<b>Hyperparameter Tuning Using Grid Search</b>\" for decide number of neurons.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "09e89eb3-1397-4e75-9dd3-511f2ca72792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden size: 2, Validation Loss: 5.2197489738464355\n",
      "Hidden size: 4, Validation Loss: 5.365090847015381\n",
      "Hidden size: 8, Validation Loss: 0.13457150757312775\n",
      "Hidden size: 16, Validation Loss: 0.09531105309724808\n",
      "Hidden size: 32, Validation Loss: 0.045698389410972595\n",
      "Hidden size: 64, Validation Loss: 0.010166753083467484\n",
      "\n",
      " Best hidden size: 64, Best validation loss: 0.010166753083467484\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Dummy dataset\n",
    "X = torch.tensor([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0], [4.0, 5.0]])\n",
    "y = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# Define model class\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.hidden = nn.Linear(2, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate(hidden_size):\n",
    "    model = SimpleNet(hidden_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val)\n",
    "        val_loss = mean_squared_error(y_val.numpy(), val_outputs.numpy())\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "# Hyperparameter tuning\n",
    "hidden_sizes = [2, 4, 8, 16, 32, 64]\n",
    "best_hidden_size = None\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    val_loss = train_and_evaluate(hidden_size)\n",
    "    print(f'Hidden size: {hidden_size}, Validation Loss: {val_loss}')\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_hidden_size = hidden_size\n",
    "\n",
    "print(f'\\n Best hidden size: {best_hidden_size}, Best validation loss: {best_val_loss}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
