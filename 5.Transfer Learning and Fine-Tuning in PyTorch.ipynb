{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a9f9809-66b7-46da-859e-81aa6c9dfef2",
   "metadata": {},
   "source": [
    "<h1>Transfer Learning and Fine-Tuning in PyTorch</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781f000a-a1bd-40de-a9fe-e5069a857695",
   "metadata": {},
   "source": [
    "<b>Transfer learning</b> leverages pre-trained models to improve the performance of a model on a new, but related task.<br>\n",
    "This technique is particularly useful when you have a limited amount of data for your specific task. <br>\n",
    "PyTorch provides several pre-trained models in the <b>torchvision.models</b> module, which can be fine-tuned for various tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f706bd-b6c3-483d-ad05-d2d0b1409f66",
   "metadata": {},
   "source": [
    "<h4>What is Transfer Learning?</h4>\n",
    "Transfer learning involves using a model trained on a large dataset for a new, but similar task.<br>\n",
    "The idea is to utilize the feature representations learned by the model on the original task and adapt them to the new task. <br>\n",
    "This typically involves:\n",
    "<ul>\n",
    "<li><b>Feature Extraction:</b> Using the pre-trained model as a fixed feature extractor.</li>\n",
    "<li><b>Fine-Tuning:</b> Updating the weights of the pre-trained model to fit the new task.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391099f3-b6ca-4f0f-8436-cb7c1f4270e9",
   "metadata": {},
   "source": [
    "<h4>Steps for Transfer Learning</h4>\n",
    "<ol>\n",
    "<li><b>Load a Pre-trained Model:</b> Use models available in <b>torchvision.models</b>.</li>\n",
    "<li><b>Modify the Model:</b> Adapt the model to your specific task.</li>\n",
    "<li><b>Prepare the Data:</b> Use appropriate data loaders for your task.</li>\n",
    "<li><b>Train the Model:</b> Fine-tune the model on your dataset.</li>\n",
    "<li><b>Evaluate the Model:</b> Assess the performance on a test set.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed75ccb-6ad5-433e-a8e1-e0b847d977d7",
   "metadata": {},
   "source": [
    "<h4>Transfer Learning with a Pre-Trained ResNet Model on CIFAR-10</h4>\n",
    " fine-tuning a pre-trained ResNet model on the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f48d764-6426-49d0-bd7f-038c8a98b3b5",
   "metadata": {},
   "source": [
    "<h5> Step 1 : Import libraries </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b11182-b46b-4e8e-a7c4-d3472167d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5597c87f-e432-4ff9-a5db-101b3a321a6d",
   "metadata": {},
   "source": [
    "<h5> Step 2: Operations on Image </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b60a3bb0-9641-4935-b893-63a54c60120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for training and testing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),                  # Resize images to 256x256\n",
    "    transforms.CenterCrop(224),              # Crop center 224x224\n",
    "    transforms.ToTensor(),                   # Convert image to tensor\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Normalize using CIFAR-10 stats\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cab5d5-5c4a-40d4-9230-ec84b2e84feb",
   "metadata": {},
   "source": [
    "<h5> Step 3: Load the dataset</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b065f5d9-ee75-4d45-a935-83a954c3e5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac102ed-fab6-420f-866f-314c8356a28b",
   "metadata": {},
   "source": [
    "<h5> Step 4: Create a subset of datast </h5>\n",
    "As we are running on CPU, the smaller set of dataset is preferable. Due to that, we are making subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6e3cf029-0202-4f6e-9e25-8fb421e3512e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices =  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "# Create a subset of 100 images from the training dataset\n",
    "indices = list(range(100))  # Adjusted to include 100 samples\n",
    "print(\"indices = \",indices)\n",
    "\n",
    "trainset = Subset(full_trainset, indices)\n",
    "batch_size = 10\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8b0f58a1-0f87-4770-861e-7495fb724f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "indices =  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10 test dataset\n",
    "full_testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "indices = list(range(10))\n",
    "print(\"indices = \",indices)\n",
    "\n",
    "testset = Subset(full_testset, indices)\n",
    "testloader = DataLoader(testset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c700266-df12-4bd6-81cf-b41111e9d832",
   "metadata": {},
   "source": [
    "<h5>Step 5: Use pre=trained ResNet model</h5>\n",
    "Transfer learning leverages a model that has been pre-trained on a large dataset (e.g., ImageNet) to solve a related task. Instead of training a model from scratch, you use the learned features of a pre-trained model to improve performance on a new dataset with potentially fewer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c8420dba-6570-4076-9aa4-fc2e1ab5fa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained ResNet model\n",
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242aac97-a74d-4147-ba9e-2afb15edc162",
   "metadata": {},
   "source": [
    "<h5> Step 6: Modify the pre-trained ResNet model</h5>\n",
    "\n",
    "<b>Fine-tuning</b> involves adjusting the pre-trained model on a new dataset by continuing the training process. The goal is to adapt the modelâ€™s features to the specific characteristics of the new dataset.<br>\n",
    "\n",
    "The <b>final layer of the pre-trained ResNet model</b> is modified to match the number of classes in the new dataset (CIFAR-10). This step adapts the model to the specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "75c512f8-6229-49de-8068-d401fdf651e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the model for CIFAR-10 (10 classes)\n",
    "num_classes = 10\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "81f491c5-dd7a-4f98-ae9c-85d505903c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2585ca0-5323-434e-923e-a03d5be24f66",
   "metadata": {},
   "source": [
    "<h5>Step 7: Loss function and Optimizer </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "bd486d3a-3bcb-4a82-bf65-b534b6f93449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "47f33572-ed80-40ea-838b-91b61c752418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a3d508-6047-44c4-af39-80a2336ae3a7",
   "metadata": {},
   "source": [
    "<h5> Step 8: Training </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "07485750-52f1-4b5b-bf1d-4faf5cfec825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "End of Epoch 1, Average Loss: 0.2321\n",
      "Epoch 2/10\n",
      "End of Epoch 2, Average Loss: 0.1961\n",
      "Epoch 3/10\n",
      "End of Epoch 3, Average Loss: 0.1499\n",
      "Epoch 4/10\n",
      "End of Epoch 4, Average Loss: 0.1348\n",
      "Epoch 5/10\n",
      "End of Epoch 5, Average Loss: 0.0735\n",
      "Epoch 6/10\n",
      "End of Epoch 6, Average Loss: 0.1115\n",
      "Epoch 7/10\n",
      "End of Epoch 7, Average Loss: 0.0705\n",
      "Epoch 8/10\n",
      "End of Epoch 8, Average Loss: 0.0534\n",
      "Epoch 9/10\n",
      "End of Epoch 9, Average Loss: 0.0485\n",
      "Epoch 10/10\n",
      "End of Epoch 10, Average Loss: 0.0525\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    avg_loss = running_loss / len(trainloader)\n",
    "    print(f'End of Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51929078-573a-482a-bf77-142b4aab9339",
   "metadata": {},
   "source": [
    "<h5> Step 9: Testing </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041d36d3-807d-47e0-be1d-29ffbdab1436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10 test images: {100 * correct / total:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
